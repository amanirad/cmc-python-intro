{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c84dba",
   "metadata": {},
   "source": [
    "# Notebook 04: Logit Model from Scratch (Swiss Route Choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec4841",
   "metadata": {},
   "source": [
    "**Objective:** Step-by-step construction of a Multinomial Logit (MNL) discrete choice model using the Apollo Swiss route choice dataset. We will go from computing utilities and choice probabilities to constructing the log-likelihood and discussing estimation (though we might not do full optimization manually, we'll illustrate the concept). Key concepts introduced include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3dc06b",
   "metadata": {},
   "source": [
    "- Utility specification for route choice (with travel time, cost, etc.).\n",
    "\n",
    "- The log-likelihood function for model estimation.\n",
    "\n",
    "- Calculating Value of Time (VOT) from model coefficients.\n",
    "\n",
    "- Evaluating model fit via metrics like Log-Likelihood and McFadden's $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4933d1c1",
   "metadata": {},
   "source": [
    "The Swiss route choice dataset `apollo_swissRouteChoiceData.csv` comes from a stated preference survey in Switzerland. Each respondent chose between two hypothetical routes for a trip. The attributes include travel time, travel cost, headway (for public transport, likely a train scenario), and number of interchanges. Additional data: car availability and trip purpose are given, but we will start with a simple model using time and cost as key attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57acbf63",
   "metadata": {},
   "source": [
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede247cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded route choice data: (3492, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>choice</th>\n",
       "      <th>tt1</th>\n",
       "      <th>tc1</th>\n",
       "      <th>hw1</th>\n",
       "      <th>ch1</th>\n",
       "      <th>tt2</th>\n",
       "      <th>tc2</th>\n",
       "      <th>hw2</th>\n",
       "      <th>ch2</th>\n",
       "      <th>hh_inc_abs</th>\n",
       "      <th>car_availability</th>\n",
       "      <th>commute</th>\n",
       "      <th>shopping</th>\n",
       "      <th>business</th>\n",
       "      <th>leisure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2439</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2439</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  choice  tt1  tc1  hw1  ch1  tt2  tc2  hw2  ch2  hh_inc_abs  \\\n",
       "0  2439       2   58    7   30    1   50    8   30    0       50000   \n",
       "1  2439       1   30    8   60    0   41    7   15    2       50000   \n",
       "2  2439       1   41    7   30    0   34    8   15    2       50000   \n",
       "\n",
       "   car_availability  commute  shopping  business  leisure  \n",
       "0                 1        1         0         0        0  \n",
       "1                 1        1         0         0        0  \n",
       "2                 1        1         0         0        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_route = pd.read_csv(\"../data/raw/apollo_swissRouteChoiceData.csv\")\n",
    "print(\"Loaded route choice data:\", df_route.shape)\n",
    "df_route.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd9b14",
   "metadata": {},
   "source": [
    "We see columns `tt1`, `tc1`, `hw1`, `ch1` for alternative 1 and `tt2`, `tc2`, `hw2`, `ch2` for alternative 2, plus `choice` (1 or 2), and `ID`, `car_availability`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e29e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3492 entries, 0 to 3491\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   ID                3492 non-null   int64\n",
      " 1   choice            3492 non-null   int64\n",
      " 2   tt1               3492 non-null   int64\n",
      " 3   tc1               3492 non-null   int64\n",
      " 4   hw1               3492 non-null   int64\n",
      " 5   ch1               3492 non-null   int64\n",
      " 6   tt2               3492 non-null   int64\n",
      " 7   tc2               3492 non-null   int64\n",
      " 8   hw2               3492 non-null   int64\n",
      " 9   ch2               3492 non-null   int64\n",
      " 10  hh_inc_abs        3492 non-null   int64\n",
      " 11  car_availability  3492 non-null   int64\n",
      " 12  commute           3492 non-null   int64\n",
      " 13  shopping          3492 non-null   int64\n",
      " 14  business          3492 non-null   int64\n",
      " 15  leisure           3492 non-null   int64\n",
      "dtypes: int64(16)\n",
      "memory usage: 436.6 KB\n"
     ]
    }
   ],
   "source": [
    "df_route.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2f112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3492, 16)\n",
      "Columns: ['ID', 'choice', 'tt1', 'tc1', 'hw1', 'ch1', 'tt2', 'tc2', 'hw2', 'ch2', 'hh_inc_abs', 'car_availability', 'commute', 'shopping', 'business', 'leisure']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>choice</th>\n",
       "      <th>tt1</th>\n",
       "      <th>tc1</th>\n",
       "      <th>hw1</th>\n",
       "      <th>ch1</th>\n",
       "      <th>tt2</th>\n",
       "      <th>tc2</th>\n",
       "      <th>hw2</th>\n",
       "      <th>ch2</th>\n",
       "      <th>hh_inc_abs</th>\n",
       "      <th>car_availability</th>\n",
       "      <th>commute</th>\n",
       "      <th>shopping</th>\n",
       "      <th>business</th>\n",
       "      <th>leisure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "      <td>3492.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22180.878866</td>\n",
       "      <td>1.503436</td>\n",
       "      <td>52.588488</td>\n",
       "      <td>19.667526</td>\n",
       "      <td>32.478522</td>\n",
       "      <td>0.939863</td>\n",
       "      <td>52.472795</td>\n",
       "      <td>19.694731</td>\n",
       "      <td>32.375430</td>\n",
       "      <td>0.945876</td>\n",
       "      <td>76507.731959</td>\n",
       "      <td>0.378866</td>\n",
       "      <td>0.286082</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.092784</td>\n",
       "      <td>0.538660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15913.386164</td>\n",
       "      <td>0.500060</td>\n",
       "      <td>46.874473</td>\n",
       "      <td>22.295193</td>\n",
       "      <td>18.488605</td>\n",
       "      <td>0.809456</td>\n",
       "      <td>46.622205</td>\n",
       "      <td>22.574277</td>\n",
       "      <td>18.470759</td>\n",
       "      <td>0.799020</td>\n",
       "      <td>44364.765243</td>\n",
       "      <td>0.485174</td>\n",
       "      <td>0.451993</td>\n",
       "      <td>0.275125</td>\n",
       "      <td>0.290170</td>\n",
       "      <td>0.498575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2439.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15308.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18533.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21948.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>112500.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>84525.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>167500.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID       choice          tt1          tc1          hw1  \\\n",
       "count   3492.000000  3492.000000  3492.000000  3492.000000  3492.000000   \n",
       "mean   22180.878866     1.503436    52.588488    19.667526    32.478522   \n",
       "std    15913.386164     0.500060    46.874473    22.295193    18.488605   \n",
       "min     2439.000000     1.000000     2.000000     1.000000    15.000000   \n",
       "25%    15308.000000     1.000000    18.000000     5.000000    15.000000   \n",
       "50%    18533.000000     2.000000    37.000000    11.000000    30.000000   \n",
       "75%    21948.250000     2.000000    75.000000    26.000000    60.000000   \n",
       "max    84525.000000     2.000000   389.000000   206.000000    60.000000   \n",
       "\n",
       "               ch1          tt2          tc2          hw2          ch2  \\\n",
       "count  3492.000000  3492.000000  3492.000000  3492.000000  3492.000000   \n",
       "mean      0.939863    52.472795    19.694731    32.375430     0.945876   \n",
       "std       0.809456    46.622205    22.574277    18.470759     0.799020   \n",
       "min       0.000000     2.000000     1.000000    15.000000     0.000000   \n",
       "25%       0.000000    18.000000     5.000000    15.000000     0.000000   \n",
       "50%       1.000000    36.500000    11.000000    30.000000     1.000000   \n",
       "75%       2.000000    74.000000    25.000000    60.000000     2.000000   \n",
       "max       2.000000   385.000000   268.000000    60.000000     2.000000   \n",
       "\n",
       "          hh_inc_abs  car_availability      commute     shopping     business  \\\n",
       "count    3492.000000       3492.000000  3492.000000  3492.000000  3492.000000   \n",
       "mean    76507.731959          0.378866     0.286082     0.082474     0.092784   \n",
       "std     44364.765243          0.485174     0.451993     0.275125     0.290170   \n",
       "min     10000.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "25%     50000.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "50%     70000.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "75%    112500.000000          1.000000     1.000000     0.000000     0.000000   \n",
       "max    167500.000000          1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "           leisure  \n",
       "count  3492.000000  \n",
       "mean      0.538660  \n",
       "std       0.498575  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       1.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape:\", df_route.shape)\n",
    "print(\"Columns:\", df_route.columns.tolist())\n",
    "df_route.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947624ed",
   "metadata": {},
   "source": [
    "**Data context:** 388 individuals, 3492 observations (choices). Each choice is between 2 alternatives described by those attributes. For example, alt1 and alt2 correspond to two different routes (say a fast expensive route vs a slow cheap route). The task is to model the probability of choosing alt1 vs alt2 as a function of attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564be025",
   "metadata": {},
   "source": [
    "## 04.1 Utility Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae7210",
   "metadata": {},
   "source": [
    "We'll assume a simple linear utility form for each alternative:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d3a08",
   "metadata": {},
   "source": [
    "$$U_{alt1} = \\beta_{time} \\; . \\; tt1 + \\beta_{cost} \\; . \\; tc1 + \\beta_{hw} \\; . \\; hw1 + \\beta_{ch} \\; . \\; ch1 + \\beta_{ASC}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30411e6a",
   "metadata": {},
   "source": [
    "$$U_{alt2} = \\beta_{time} \\; . \\; tt2 + \\beta_{cost} \\; . \\; tc2 + \\beta_{hw} \\; . \\; hw2 + \\beta_{ch} \\; . \\; ch2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2eefe",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "- $\\beta_{time}$ ​is the coefficient for travel time (per minute, expected negative).\n",
    "\n",
    "- $\\beta_{cost}$ for travel cost (per CHF, expected negative).\n",
    "\n",
    "- $\\beta_{hw}$ for headway (minutes between services, only relevant if these are public transit routes, expected negative).\n",
    "\n",
    "- ​$\\beta_{ch}$ for number of interchanges (transfers, expected negative).\n",
    "\n",
    "- $\\beta_{ASC}$ ​is an alternative-specific constant (ASC) for alt1, to capture any inherent preference for alt1 not explained by attributes. We set ASC for alt2 as 0 for identification (so $\\beta_{ASC}$ effectively measures preference for alt1 relative to alt2). In binary choice, one can include an ASC for one alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8167c688",
   "metadata": {},
   "source": [
    "At first, let's consider a simpler model ignoring headway and interchanges (or lump them into error). But the data likely expects those to be used. We can include them for completeness since their interpretation is straightforward (disutility for waiting time and transfers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915dd50",
   "metadata": {},
   "source": [
    "We will not estimate these coefficients from scratch by gradient methods here (that would be complex for this format), but we can demonstrate calculation of log-likelihood for given parameters and perhaps do a simple manual search or reasoning. Suppose initial guesses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc50a4",
   "metadata": {},
   "source": [
    "- $\\beta_{time}$ ​= -0.1 (utility per minute),\n",
    "\n",
    "- $\\beta_{cost}$ = -1.0 (utility per CHF, since cost often has bigger coefficient magnitude if currency in CHF, an initial guess might be that 10 CHF ~ 10 minutes equivalent? We'll see).\n",
    "\n",
    "- $\\beta_{hw}$ = -0.05 (per minute of headway),\n",
    "\n",
    "- ​$\\beta_{ch}$ = -1.0 (per interchange),\n",
    "\n",
    "- $\\beta_{ASC}$ = 0 (no inherent bias to start)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47c8ef",
   "metadata": {},
   "source": [
    "We can compute the log-likelihood of the model with these guesses and then discuss adjusting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7409dfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tt1          tc1          tt2          tc2\n",
      "count  3492.000000  3492.000000  3492.000000  3492.000000\n",
      "mean     52.588488    19.667526    52.472795    19.694731\n",
      "std      46.874473    22.295193    46.622205    22.574277\n",
      "min       2.000000     1.000000     2.000000     1.000000\n",
      "25%      18.000000     5.000000    18.000000     5.000000\n",
      "50%      37.000000    11.000000    36.500000    11.000000\n",
      "75%      75.000000    26.000000    74.000000    25.000000\n",
      "max     389.000000   206.000000   385.000000   268.000000\n"
     ]
    }
   ],
   "source": [
    "# Inspect variable scales first\n",
    "print(df_route[['tt1','tc1','tt2','tc2']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db327cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood with initial guess: -1738.424356840138\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "beta_time = -0.05\n",
    "beta_cost = -0.5\n",
    "beta_hw = -0.05\n",
    "beta_ch = -1.0\n",
    "beta_ASC = 0.0  # ASC for alt1\n",
    "\n",
    "# rescale cost if tc is in cents/large integers -> convert to CHF\n",
    "df_route['tc1_chf'] = df_route['tc1'] / 10.0\n",
    "df_route['tc2_chf'] = df_route['tc2'] / 10.0\n",
    "\n",
    "\n",
    "# Compute utilities for each alternative for all observations\n",
    "V1 = (beta_time * df_route[\"tt1\"] + beta_cost * df_route[\"tc1_chf\"] + \n",
    "      beta_hw * df_route[\"hw1\"] + beta_ch * df_route[\"ch1\"] + beta_ASC)\n",
    "V2 = (beta_time * df_route[\"tt2\"] + beta_cost * df_route[\"tc2_chf\"] + \n",
    "      beta_hw * df_route[\"hw2\"] + beta_ch * df_route[\"ch2\"] + 0)  # ASC for alt2 = 0\n",
    "\n",
    "# Choice probabilities for each observation\n",
    "# Use numerically stable logit by subtracting the max utility to avoid overflow\n",
    "import numpy as np\n",
    "maxV = np.maximum(V1, V2)\n",
    "expV1 = np.exp(V1 - maxV)\n",
    "expV2 = np.exp(V2 - maxV)\n",
    "P1 = expV1 / (expV1 + expV2)\n",
    "P2 = expV2 / (expV1 + expV2)\n",
    "\n",
    "# Avoid log(0) by clipping probabilities\n",
    "eps = 1e-12\n",
    "P1 = np.clip(P1, eps, 1 - eps)\n",
    "P2 = np.clip(P2, eps, 1 - eps)\n",
    "\n",
    "# Now compute log-likelihood\n",
    "chosen = df_route[\"choice\"]  # 1 or 2\n",
    "# If choice==1, contribution = log(P1); if choice==2, log(P2)\n",
    "log_likelihood = np.where(chosen == 1, np.log(P1), np.log(P2)).sum()\n",
    "print(\"Log-likelihood with initial guess:\", log_likelihood)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692853e5",
   "metadata": {},
   "source": [
    "For context, the null log-likelihood (no predictors, just probability 0.5 each since two options) would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e671775c",
   "metadata": {},
   "source": [
    "$$LL_{null} = N \\, . \\, \\ln(0.5)$$\n",
    "\n",
    "where N = 3492 observations. That would be 3492 * ln(0.5) $\\approx$ -2421.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05dc3c3",
   "metadata": {},
   "source": [
    "Our model's log-likelihood with initial guess might be better (less negative) if the guess is somewhat reasonable. The aim in estimation is to find $\\beta$'s that maximize this log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386e286",
   "metadata": {},
   "source": [
    "At the moment, we won’t run an optimizer here, but we can conceptually discuss which direction to move parameters:\n",
    "\n",
    "- If we increase $|\\beta_{time}|$ (make it more negative), that means we give more disutility to longer travel time. If in the data people often chose the route with less travel time, a more negative $\\beta_{time}$ will boost probability of the shorter route, improving likelihood.\n",
    "\n",
    "- If cost coefficient is off, say initial -1.0 might be too high or too low. Value of Time concept can guide adjusting both together (we discuss VOT soon).\n",
    "\n",
    "- The ASC might not be zero in reality; if say alt1 was chosen more often even after accounting for attributes, $\\beta_{ASC}$ would adjust positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d8bef",
   "metadata": {},
   "source": [
    "Let’s try a pseudo-optimization by intuition or brute force on one parameter (not too computational heavy as 3492 obs):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc519f1",
   "metadata": {},
   "source": [
    "We could loop over, say, $\\beta_{time}$ values and see how log-likelihood changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b0809d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_time=-0.01: logLik=-1807.0\n",
      "beta_time=-0.05: logLik=-1738.4\n",
      "beta_time=-0.1: logLik=-1985.4\n",
      "beta_time=-0.2: logLik=-3099.4\n",
      "beta_time=-0.3: logLik=-4572.6\n"
     ]
    }
   ],
   "source": [
    "for bt in [-0.01, -0.05, -0.1, -0.2, -0.3]:\n",
    "    V1 = bt*df_route[\"tt1\"] + beta_cost*df_route[\"tc1_chf\"] + beta_hw*df_route[\"hw1\"] + beta_ch*df_route[\"ch1\"]\n",
    "    V2 = bt*df_route[\"tt2\"] + beta_cost*df_route[\"tc2_chf\"] + beta_hw*df_route[\"hw2\"] + beta_ch*df_route[\"ch2\"]\n",
    "    expV1, expV2 = np.exp(V1), np.exp(V2)\n",
    "    P1 = expV1/(expV1+expV2)\n",
    "    loglik = np.where(chosen==1, np.log(P1), np.log(1-P1)).sum()\n",
    "    print(f\"beta_time={bt}: logLik={loglik:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee30681",
   "metadata": {},
   "source": [
    "This will show which $\\beta_{time}$ yields higher logLik (less negative). We expect a certain negative value around optimum. A rigorous approach would vary all parameters, but for brevity, let's assume we find something like $\\beta_{time} \\approx -0.05$, $\\beta_{cost} \\approx -0.8$, $\\beta_{hw} \\approx -0.1$, $\\beta_{ch} \\approx -0.2$, $\\beta_{ASC} \\approx ???$ yields a good fit. (These are guessy; let's focus on concepts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573fee5",
   "metadata": {},
   "source": [
    "## 04.2 Value of Time (VOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cee10",
   "metadata": {},
   "source": [
    "Value of Time is a key derivative from logit models: it is the trade-off between time and cost, i.e., \n",
    "\n",
    "$$VOT = -\\frac{\\beta_{time}}{\\beta_{cost}}$$\n",
    "\n",
    "in currency units per time (e.g., CHF per minute, which can be converted to CHF per hour by $\\times 60$). Essentially, how much money a traveler is willing to pay to save time (the marginal rate of substitution between time and cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5af3d5",
   "metadata": {},
   "source": [
    "Once we have estimated $\\beta_{time}$ and $\\beta_{cost}$, we compute VOT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f5c8ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of Time: -0.06 CHF per minute (-3.75 CHF per hour)\n"
     ]
    }
   ],
   "source": [
    "beta_time_est = -0.05  # assume from estimation\n",
    "beta_cost_est = -0.8  # assume from estimation\n",
    "VOT_minutes = - beta_time_est / beta_cost_est  # in CHF per minute\n",
    "VOT_hours = VOT_minutes * 60\n",
    "print(f\"Value of Time: {VOT_minutes:.2f} CHF per minute ({VOT_hours:.2f} CHF per hour)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e7818",
   "metadata": {},
   "source": [
    "If $\\beta_{time} = -0.05$, $\\beta_{cost} = -0.8$, then $VOT = -(-0.0.05/-0.8) = -0.06$ CHF/minute, which is 3.75 CHF/hour. That would be a plausible number depending on context. If many business travelers, VOT might be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85c218",
   "metadata": {},
   "source": [
    "Interpretation: A VOT of 3.75 CHF/hour means, on average, travelers are willing to pay 3.75 CHF to save one hour of travel time. If our model found that, it quantifies the trade-off in a single metric. Typically, one compares that to wage rates or policy values to see if it makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06830d74",
   "metadata": {},
   "source": [
    "## 04.3 Model Fit and McFadden's $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbb9e1",
   "metadata": {},
   "source": [
    "\n",
    "Let's fit the model by minimizing the negative log‑likelihood using BFGS on the original (unstandardized) predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a71af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 1665.619946\n",
      "         Iterations: 15\n",
      "         Function evaluations: 168\n",
      "         Gradient evaluations: 28\n",
      "Estimates (bt, bc, bhw, bch, ASC): [-0.0598 -1.3173 -0.0374 -1.1521 -0.0159]\n",
      "Std. errors: [0.0064 0.0964 0.0051 0.0785 0.0416]\n",
      "Log-likelihood (final): -1665.6199462956301\n",
      "Null log-likelihood: -2420.469954515329\n",
      "McFadden R^2: 0.3119\n",
      "VOT: -0.0454 CHF/min (-2.72 CHF/hour)\n",
      "Prediction accuracy: 0.7864\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 1665.619946\n",
      "         Iterations: 15\n",
      "         Function evaluations: 168\n",
      "         Gradient evaluations: 28\n",
      "Estimates (bt, bc, bhw, bch, ASC): [-0.0598 -1.3173 -0.0374 -1.1521 -0.0159]\n",
      "Std. errors: [0.0064 0.0964 0.0051 0.0785 0.0416]\n",
      "Log-likelihood (final): -1665.6199462956301\n",
      "Null log-likelihood: -2420.469954515329\n",
      "McFadden R^2: 0.3119\n",
      "VOT: -0.0454 CHF/min (-2.72 CHF/hour)\n",
      "Prediction accuracy: 0.7864\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# ensure cost is scaled (adjust divisor if your describe() suggests different scale)\n",
    "if \"tc1_chf\" not in df_route.columns:\n",
    "    df_route[\"tc1_chf\"] = df_route[\"tc1\"] / 10.0\n",
    "    df_route[\"tc2_chf\"] = df_route[\"tc2\"] / 10.0\n",
    "\n",
    "chosen = df_route[\"choice\"].to_numpy()\n",
    "\n",
    "def neg_loglike(params):\n",
    "    bt, bc, bhw, bch, basc = params\n",
    "    V1 = bt * df_route[\"tt1\"].to_numpy() + bc * df_route[\"tc1_chf\"].to_numpy() + bhw * df_route[\"hw1\"].to_numpy() + bch * df_route[\"ch1\"].to_numpy() + basc\n",
    "    V2 = bt * df_route[\"tt2\"].to_numpy() + bc * df_route[\"tc2_chf\"].to_numpy() + bhw * df_route[\"hw2\"].to_numpy() + bch * df_route[\"ch2\"].to_numpy()\n",
    "    # numerically stable log-likelihood (log-sum-exp)\n",
    "    maxV = np.maximum(V1, V2)\n",
    "    log_denom = maxV + np.log(np.exp(V1 - maxV) + np.exp(V2 - maxV))\n",
    "    logP1 = V1 - log_denom\n",
    "    logP2 = V2 - log_denom\n",
    "    ll = np.where(chosen == 1, logP1, logP2).sum()\n",
    "    return -ll  # minimize negative log-likelihood\n",
    "\n",
    "# starting values\n",
    "start = np.array([-0.05, -0.5, -0.05, -1.0, 0.0])\n",
    "\n",
    "res = minimize(neg_loglike, start, method=\"BFGS\", options={\"disp\": True})\n",
    "\n",
    "# results\n",
    "params_est = res.x\n",
    "ll_final = -res.fun\n",
    "N = len(df_route)\n",
    "null_ll = N * np.log(0.5)\n",
    "mcff_r2 = 1 - (ll_final / null_ll)\n",
    "\n",
    "# approximate std errors from inverse Hessian (BFGS provides hess_inv)\n",
    "if hasattr(res, \"hess_inv\") and hasattr(res.hess_inv, \"todense\") is False:\n",
    "    cov = res.hess_inv\n",
    "else:\n",
    "    try:\n",
    "        cov = res.hess_inv.todense()\n",
    "    except Exception:\n",
    "        cov = None\n",
    "\n",
    "se = np.sqrt(np.diag(cov)) if cov is not None else np.full_like(params_est, np.nan)\n",
    "\n",
    "print(\"Estimates (bt, bc, bhw, bch, ASC):\", np.round(params_est, 4))\n",
    "print(\"Std. errors:\", np.round(se, 4))\n",
    "print(\"Log-likelihood (final):\", ll_final)\n",
    "print(\"Null log-likelihood:\", null_ll)\n",
    "print(\"McFadden R^2:\", np.round(mcff_r2, 4))\n",
    "\n",
    "# Value of Time in CHF per minute and per hour\n",
    "beta_time_est, beta_cost_est = params_est[0], params_est[1]\n",
    "vot_min = -beta_time_est / beta_cost_est\n",
    "vot_hr = vot_min * 60\n",
    "print(f\"VOT: {vot_min:.4f} CHF/min ({vot_hr:.2f} CHF/hour)\")\n",
    "\n",
    "# simple prediction accuracy\n",
    "V1 = beta_time_est * df_route[\"tt1\"] + beta_cost_est * df_route[\"tc1_chf\"] + params_est[2] * df_route[\"hw1\"] + params_est[3] * df_route[\"ch1\"] + params_est[4]\n",
    "V2 = beta_time_est * df_route[\"tt2\"] + beta_cost_est * df_route[\"tc2_chf\"] + params_est[2] * df_route[\"hw2\"] + params_est[3] * df_route[\"ch2\"]\n",
    "pred = np.where(V1 > V2, 1, 2)\n",
    "acc = (pred == df_route[\"choice\"]).mean()\n",
    "print(\"Prediction accuracy:\", np.round(acc, 4))\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# ensure cost is scaled (adjust divisor if your describe() suggests different scale)\n",
    "if \"tc1_chf\" not in df_route.columns:\n",
    "    df_route[\"tc1_chf\"] = df_route[\"tc1\"] / 10.0\n",
    "    df_route[\"tc2_chf\"] = df_route[\"tc2\"] / 10.0\n",
    "\n",
    "chosen = df_route[\"choice\"].to_numpy()\n",
    "\n",
    "def neg_loglike(params):\n",
    "    bt, bc, bhw, bch, basc = params\n",
    "    V1 = bt * df_route[\"tt1\"].to_numpy() + bc * df_route[\"tc1_chf\"].to_numpy() + bhw * df_route[\"hw1\"].to_numpy() + bch * df_route[\"ch1\"].to_numpy() + basc\n",
    "    V2 = bt * df_route[\"tt2\"].to_numpy() + bc * df_route[\"tc2_chf\"].to_numpy() + bhw * df_route[\"hw2\"].to_numpy() + bch * df_route[\"ch2\"].to_numpy()\n",
    "    # numerically stable log-likelihood (log-sum-exp)\n",
    "    maxV = np.maximum(V1, V2)\n",
    "    log_denom = maxV + np.log(np.exp(V1 - maxV) + np.exp(V2 - maxV))\n",
    "    logP1 = V1 - log_denom\n",
    "    logP2 = V2 - log_denom\n",
    "    ll = np.where(chosen == 1, logP1, logP2).sum()\n",
    "    return -ll  # minimize negative log-likelihood\n",
    "\n",
    "# starting values\n",
    "start = np.array([-0.05, -0.5, -0.05, -1.0, 0.0])\n",
    "\n",
    "res = minimize(neg_loglike, start, method=\"BFGS\", options={\"disp\": True})\n",
    "\n",
    "# results\n",
    "params_est = res.x\n",
    "ll_final = -res.fun\n",
    "N = len(df_route)\n",
    "null_ll = N * np.log(0.5)\n",
    "mcff_r2 = 1 - (ll_final / null_ll)\n",
    "\n",
    "# approximate std errors from inverse Hessian (BFGS provides hess_inv)\n",
    "if hasattr(res, \"hess_inv\") and hasattr(res.hess_inv, \"todense\") is False:\n",
    "    cov = res.hess_inv\n",
    "else:\n",
    "    try:\n",
    "        cov = res.hess_inv.todense()\n",
    "    except Exception:\n",
    "        cov = None\n",
    "\n",
    "se = np.sqrt(np.diag(cov)) if cov is not None else np.full_like(params_est, np.nan)\n",
    "\n",
    "print(\"Estimates (bt, bc, bhw, bch, ASC):\", np.round(params_est, 4))\n",
    "print(\"Std. errors:\", np.round(se, 4))\n",
    "print(\"Log-likelihood (final):\", ll_final)\n",
    "print(\"Null log-likelihood:\", null_ll)\n",
    "print(\"McFadden R^2:\", np.round(mcff_r2, 4))\n",
    "\n",
    "# Value of Time in CHF per minute and per hour\n",
    "beta_time_est, beta_cost_est = params_est[0], params_est[1]\n",
    "vot_min = -beta_time_est / beta_cost_est\n",
    "vot_hr = vot_min * 60\n",
    "print(f\"VOT: {vot_min:.4f} CHF/min ({vot_hr:.2f} CHF/hour)\")\n",
    "\n",
    "# simple prediction accuracy\n",
    "V1 = beta_time_est * df_route[\"tt1\"] + beta_cost_est * df_route[\"tc1_chf\"] + params_est[2] * df_route[\"hw1\"] + params_est[3] * df_route[\"ch1\"] + params_est[4]\n",
    "V2 = beta_time_est * df_route[\"tt2\"] + beta_cost_est * df_route[\"tc2_chf\"] + params_est[2] * df_route[\"hw2\"] + params_est[3] * df_route[\"ch2\"]\n",
    "pred = np.where(V1 > V2, 1, 2)\n",
    "acc = (pred == df_route[\"choice\"]).mean()\n",
    "print(\"Prediction accuracy:\", np.round(acc, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b7e29",
   "metadata": {},
   "source": [
    "Now, let's repeat estimation but standardizes time and cost before optimization and uses L‑BFGS‑B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d73548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            5     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.95807D+03    |proj g|=  1.28487D+03\n",
      "\n",
      "At iterate    1    f=  1.95563D+03    |proj g|=  2.17328D+02\n",
      "\n",
      "At iterate    2    f=  1.95249D+03    |proj g|=  7.04387D+02\n",
      "\n",
      "At iterate    3    f=  1.93917D+03    |proj g|=  2.02540D+03\n",
      "\n",
      "At iterate    4    f=  1.89941D+03    |proj g|=  4.32160D+03\n",
      "\n",
      "At iterate    5    f=  1.83745D+03    |proj g|=  5.53711D+03\n",
      "\n",
      "At iterate    6    f=  1.76980D+03    |proj g|=  4.33477D+03\n",
      "\n",
      "At iterate    7    f=  1.73797D+03    |proj g|=  2.12364D+03\n",
      "\n",
      "At iterate    8    f=  1.72935D+03    |proj g|=  4.96715D+02\n",
      "\n",
      "At iterate    9    f=  1.72814D+03    |proj g|=  2.10130D+02\n",
      "\n",
      "At iterate   10    f=  1.72777D+03    |proj g|=  3.45625D+02\n",
      "\n",
      "At iterate   11    f=  1.72687D+03    |proj g|=  6.24157D+02\n",
      "\n",
      "At iterate   12    f=  1.72468D+03    |proj g|=  1.00332D+03\n",
      "\n",
      "At iterate   13    f=  1.71904D+03    |proj g|=  1.57977D+03\n",
      "\n",
      "At iterate   14    f=  1.70682D+03    |proj g|=  2.28937D+03\n",
      "\n",
      "At iterate   15    f=  1.68823D+03    |proj g|=  2.46696D+03\n",
      "\n",
      "At iterate   16    f=  1.68703D+03    |proj g|=  2.00022D+03\n",
      "\n",
      "At iterate   17    f=  1.67434D+03    |proj g|=  9.35266D+02\n",
      "\n",
      "At iterate   18    f=  1.67198D+03    |proj g|=  1.52279D+02\n",
      "\n",
      "At iterate   19    f=  1.67186D+03    |proj g|=  9.80967D+00\n",
      "\n",
      "At iterate   20    f=  1.67185D+03    |proj g|=  9.74862D+00\n",
      "\n",
      "At iterate   21    f=  1.67185D+03    |proj g|=  2.04776D+01\n",
      "\n",
      "At iterate   22    f=  1.67183D+03    |proj g|=  4.52719D+01\n",
      "\n",
      "At iterate   23    f=  1.67180D+03    |proj g|=  8.51348D+01\n",
      "\n",
      "At iterate   24    f=  1.67171D+03    |proj g|=  1.34019D+02\n",
      "\n",
      "At iterate   25    f=  1.67156D+03    |proj g|=  1.70145D+02\n",
      "\n",
      "At iterate   26    f=  1.67138D+03    |proj g|=  1.44991D+02\n",
      "\n",
      "At iterate   27    f=  1.67121D+03    |proj g|=  5.01713D+01\n",
      "\n",
      "At iterate   28    f=  1.67110D+03    |proj g|=  2.39262D+01\n",
      "\n",
      "At iterate   29    f=  1.67107D+03    |proj g|=  1.13178D+02\n",
      "\n",
      "At iterate   30    f=  1.67102D+03    |proj g|=  1.20537D+02\n",
      "\n",
      "At iterate   31    f=  1.67063D+03    |proj g|=  1.41604D+02\n",
      "\n",
      "At iterate   32    f=  1.67036D+03    |proj g|=  1.79931D+02\n",
      "\n",
      "At iterate   33    f=  1.66898D+03    |proj g|=  1.66819D+02\n",
      "\n",
      "At iterate   34    f=  1.66566D+03    |proj g|=  6.93408D+00\n",
      "\n",
      "At iterate   35    f=  1.66562D+03    |proj g|=  6.02677D-01\n",
      "\n",
      "At iterate   36    f=  1.66562D+03    |proj g|=  3.57954D-01\n",
      "\n",
      "At iterate   37    f=  1.66562D+03    |proj g|=  3.56977D-03\n",
      "Optimization success: True\n",
      "Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "Estimated (transformed to original units): [-0.059752 -1.317323 -0.037447 -1.152118 -0.015873]\n",
      "\n",
      "At iterate   38    f=  1.66562D+03    |proj g|=  3.18323D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    5     38     46      1     0     0   3.183D-04   1.666D+03\n",
      "  F =   1665.6199462956024     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "Log-likelihood (final): -1665.6199462956024\n",
      "Null log-likelihood: -2420.469954515329\n",
      "McFadden R^2: 0.31186092882978034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- scale numeric predictors to improve conditioning ---\n",
    "# scale time and cost across both alternatives\n",
    "time_vals = pd.concat([df_route[\"tt1\"], df_route[\"tt2\"]])\n",
    "cost_vals = pd.concat([df_route[\"tc1_chf\"], df_route[\"tc2_chf\"]])\n",
    "time_scale = time_vals.std() if time_vals.std() != 0 else 1.0\n",
    "cost_scale = cost_vals.std() if cost_vals.std() != 0 else 1.0\n",
    "\n",
    "df_route[\"tt1_s\"] = df_route[\"tt1\"] / time_scale\n",
    "df_route[\"tt2_s\"] = df_route[\"tt2\"] / time_scale\n",
    "df_route[\"tc1_s\"] = df_route[\"tc1_chf\"] / cost_scale\n",
    "df_route[\"tc2_s\"] = df_route[\"tc2_chf\"] / cost_scale\n",
    "\n",
    "chosen = df_route[\"choice\"].to_numpy()\n",
    "\n",
    "def neg_loglike(params):\n",
    "    bt, bc, bhw, bch, basc = params\n",
    "    V1 = bt * df_route[\"tt1_s\"].to_numpy() + bc * df_route[\"tc1_s\"].to_numpy() + bhw * df_route[\"hw1\"].to_numpy() + bch * df_route[\"ch1\"].to_numpy() + basc\n",
    "    V2 = bt * df_route[\"tt2_s\"].to_numpy() + bc * df_route[\"tc2_s\"].to_numpy() + bhw * df_route[\"hw2\"].to_numpy() + bch * df_route[\"ch2\"].to_numpy()\n",
    "    # numerically stable log-likelihood (log-sum-exp)\n",
    "    maxV = np.maximum(V1, V2)\n",
    "    log_denom = maxV + np.log(np.exp(V1 - maxV) + np.exp(V2 - maxV))\n",
    "    logP1 = V1 - log_denom\n",
    "    logP2 = V2 - log_denom\n",
    "    ll = np.where(chosen == 1, logP1, logP2).sum()\n",
    "    return -ll\n",
    "\n",
    "# starting values adjusted for standardized predictors (smaller mags)\n",
    "start = np.array([-0.06, -1.3 * cost_scale, -0.04, -1.0, 0.0])  # if you prefer, use smaller for cost too\n",
    "\n",
    "# use L-BFGS-B (more robust for large problems) and allow more iterations\n",
    "res = minimize(neg_loglike, start, method=\"L-BFGS-B\",\n",
    "               options={\"disp\": True, \"maxiter\": 2000})\n",
    "\n",
    "# check solver status\n",
    "print(\"Optimization success:\", res.success)\n",
    "print(\"Message:\", res.message)\n",
    "\n",
    "# transform estimated betas back to original units:\n",
    "# bt (per std(time)) -> per minute: bt / time_scale\n",
    "# bc (per std(cost)) -> per CHF: bc / cost_scale\n",
    "params_std = res.x\n",
    "params_est = params_std.copy()\n",
    "params_est[0] = params_std[0] / time_scale\n",
    "params_est[1] = params_std[1] / cost_scale\n",
    "# bhw, bch, basc unchanged (they were not scaled)\n",
    "print(\"Estimated (transformed to original units):\", np.round(params_est, 6))\n",
    "\n",
    "# compute final reported metrics using transformed params\n",
    "bt, bc, bhw, bch, basc = params_est\n",
    "V1 = bt * df_route[\"tt1\"] + bc * df_route[\"tc1_chf\"] + bhw * df_route[\"hw1\"] + bch * df_route[\"ch1\"] + basc\n",
    "V2 = bt * df_route[\"tt2\"] + bc * df_route[\"tc2_chf\"] + bhw * df_route[\"hw2\"] + bch * df_route[\"ch2\"]\n",
    "maxV = np.maximum(V1, V2)\n",
    "log_denom = maxV + np.log(np.exp(V1 - maxV) + np.exp(V2 - maxV))\n",
    "logP1 = V1 - log_denom\n",
    "logP2 = V2 - log_denom\n",
    "ll_final = np.where(chosen == 1, logP1, logP2).sum()\n",
    "N = len(df_route)\n",
    "null_ll = N * np.log(0.5)\n",
    "print(\"Log-likelihood (final):\", ll_final)\n",
    "print(\"Null log-likelihood:\", null_ll)\n",
    "print(\"McFadden R^2:\", 1 - (ll_final / null_ll))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc1aac",
   "metadata": {},
   "source": [
    "In the next notebook, we will apply what we learned to a couple of mini-projects using the Apollo time use and drug choice datasets. We'll do some exploratory data analysis and maybe a simple simulation or model on each, to further solidify these skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37d174",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
